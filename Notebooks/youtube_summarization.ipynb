{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.schema import Document\n",
    "from mp4_downloader import process_youtube_video\n",
    "from transcribe import WhisperTranscriber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=0bb3-bjgf88\n",
      "[youtube] 0bb3-bjgf88: Downloading webpage\n",
      "[youtube] 0bb3-bjgf88: Downloading ios player API JSON\n",
      "[youtube] 0bb3-bjgf88: Downloading web creator player API JSON\n",
      "[youtube] 0bb3-bjgf88: Downloading m3u8 information\n",
      "[info] 0bb3-bjgf88: Downloading 1 format(s): 18\n",
      "[download] Destination: Saved_Media\\video.mp4\n",
      "[download] 100% of   21.84MiB in 00:00:01 at 13.67MiB/s    \n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=0bb3-bjgf88\n",
      "[youtube] 0bb3-bjgf88: Downloading webpage\n",
      "[youtube] 0bb3-bjgf88: Downloading ios player API JSON\n",
      "[youtube] 0bb3-bjgf88: Downloading web creator player API JSON\n",
      "[youtube] 0bb3-bjgf88: Downloading m3u8 information\n",
      "[info] 0bb3-bjgf88: Downloading 1 format(s): 251\n",
      "[download] Destination: Saved_Media\\audio.webm\n",
      "[download] 100% of    7.14MiB in 00:00:00 at 19.54MiB/s    \n",
      "Video and audio download completed!\n",
      "MoviePy - Writing audio in Saved_Media\\audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Audio conversion to MP3 completed!\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Let's get lean as Torvald's thoughts on Rust in the Linux kernel, which has been a contentious topic over the last few weeks, especially between some of the long-term maintainers and new contributors, and what their individual challenges are by bringing Rust into the Linux kernel with even one maintainer stepping down for their almost four-year role in the Rust for Linux Project, citing some non-technical nonsense that was going on, and that they were lacking the energy to deal with. Anyways, here's a recent conversation that Lean has had with Dirk, who is the head of the  the open source program office app horizon, let's get into what leanest things about rust in the Linux kernel and these recent events.  the folks from Azahi working on the Apple Silicon GFX drivers and the DRM's get people. So why is this so hard? I think I actually enjoyed. I enjoy arguments. I think one of the nice parts about Rust has been how it's live and up some of the discussions. And I mean some of the arguments get nasty and people do actually, yes, decide this is not worth my time. But at the same time it's kind of interesting and I think it shows how much people care.  At the same time, I'm not quite sure why Russ has been such contentious area. It reminds me of when I was young and people were arguing about V.I. versus E. Max. It's still R. Maybe they still R. And I've just- Should we have a poll here in the room? Yeah, no. But for some reason, the whole Russ versus C discussion has taken almost religious overchones in certain areas.  part of this is that from a design philosophy perspective, if you want to actually write memory, save, rust code, there are certain architectural requirements. And it seems that a lot of the older and not only in age, I mean just having been around for a longer time, maintainers, a fairly resistant to accepting the fact that if you have a rust subsystem, it requires you to have certain failure handling, which they don't want to advocate for  because he said, oh, that's a bug that should never happen. I mean, we have been doing this for over three decades and C is in the end a very simple language. It's one of the reasons I enjoy C and why a lot of C programmers enjoy C, even if the other side of that pictures obviously that because it's simple, it's also very easy to make mistakes. And Rust is not Rust is a very different thing and there's a lot of people who are used to the C model and they don't necessarily like the  And that's okay. In the kernel itself, almost nobody, I mean, I don't want even say almost. Absolutely nobody understands everything at the kernel. I don't. I rely heavily on maintainers of various subsistence and there are only a few areas where I get personally very much involved. And I think the same is true of Rust and C, is that you should expect that  not everybody will understand rust and on the other hand not all the sea rest people necessarily understand sea and that's okay I think it's actually one of the strengths we have in the kernel that we can specialize in some people care about drivers and very specific drivers and that at that some people care about specific architectures and some people like file systems and that's how it should be and I that's how I see rust but clearly there are people who just don't like  the notion of rust and having rust and crote on their area. And I think it's been interesting and I'm not afraid, people have even talked about the rust integration being a failure. And I'm actually, I don't think A, we've been doing this for a couple of years now, so it's way too early to even say that, but I also think even if it were to become a failure and I don't think it will, that's how you learn. So I see the whole rust thing  as positive even if the arguments are not necessarily always. Yeah, and I mean the challenge really is the idea that a memory-safe architecture makes certain assumptions about the infrastructure. And it's the infrastructure people like the RMSKET as the example it has become very public. It's the infrastructure people who seem to resist some of the changes. To be fair, I mean we've done a lot of changes to the scene infrastructure, the kind of code, the kernel data,  is not normalcy. We have, it's not just that we've right things in a certain way. It's also that we have a lot of tools on the C side that enforce infrastructure rules that are not part of the language. And in part of it, this is our locking safety. We do have actually a lot of memory safety infrastructure on the C side that is not technically part of C itself. It's part of our infrastructure for the kernel, where we  We have debug builds that make the kernel go much slower, but it will test a lot more of the memory safety stuff. So on the C-side though, we have been able to kind of incrementally add it. And I mean, we have a lot of that kind of support, but we've had a long time to add it, and that kind of has allowed people to do it without the outcry. And the rest changes obviously a much bigger and much more in your face.  And so you said that it's too early to say it's a failure and I agree with that. But one of the things that I've seen lately is a certain amount of interest in bottoms up grown up from the start rust kernels as an alternative to Linux. So is that a potential outcome if we continue to see struggle to get rust into Linux that there will be something like Redox or Mastro or one of the other ones  that is popping up, that we will have an alternative universe built around the different column. I think that's regardless of the Linux use of Rust, I think if you're doing operating systems, you really don't have very many choices when it comes to languages because you require a language that can deal with system issues and that automatically means that your language choices are very limited unless you want to write an assembler you're going to.  see or one of the sea light languages or you're likely going to use rest. And I don't think, I mean Linux is not everywhere. We're very widely spread, but after three decades Linux is also very big and we've always seen these embedded systems in particular who just want something smaller and simpler and not quite as fully fledged.  or see or anything else and that will never go away. I think. I mean, obviously, you see things like Zephyr and other embedded, deeply embedded kernels. But as a general pro-plus OS, I think we can say Linux is absolutely everywhere. It's today, there is a complete Linux distribution running on most 5G modern chips. So in UI phone is a chip that runs as it's firmware Linux. So it is everywhere.  I mean, yes, we used to have this joke. I mean, many, many, many years ago, we used to talk about the world domination, and that joke became reality and isn't funny anymore. And I'm okay with that. But at the same time, I mean, nothing lasts forever. And I'm sure some clueless young person will decide how hard can it be and start his own operating system in Rust, or in something else. And if he keeps at it for many, many, many years,  He dick or she, and many many decades. Thanks to the Linux Foundation for hosting this event in conversation. If you want more of this video or you found this conversation interesting between Linus and Dirk, check for the link in the description below. Let me know what you think about, rest in the Linux kernel, catch me in a great community on Discord and I'll catch you in another video. Thanks for watching.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_youtube_video(\"https://www.youtube.com/watch?v=0bb3-bjgf88\")\n",
    "transcriber = WhisperTranscriber()\n",
    "transcribed_text = transcriber.transcribe(r\"Saved_Media\\audio.mp3\")\n",
    "transcribed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LLM with prompt:\n",
      "<|system|> You are a helpful AI assistant that provides clear and concise information.<|end|><|user|> Write step by step the answer to 2x+3=7.<|end|><|assistant|> \n",
      "1. First, you need to understand what an equation is: it's a mathematical statement where two expressions are equal (e.g., \"5 + 4 = 9\"). In this case, we have \"2x + 3 = 7\", which means that whatever value x takes on, when substituted into the expression '2x + 3', should result in an outcome of 7.\n",
      "\n",
      "  2. Our goal here is to solve for x by isolating it on one side of the equation. To do this, we'll need to perform operations that will eliminate any other terms or numbers from its immediate vicinity but won't change our original aim (the value of 'x'). \n",
      "\n",
      "3. The first step in solving a linear equation is usually straightforward: get rid of constant terms on the same side as your variable(s). In this case, we can subtract 3 from both sides to achieve that goal. So let us do it!\n",
      "  2x + 3 - 3 = 7 - 3\n",
      "    Simplify by combining like terms and perform subtraction:\n",
      "    2x = 4\n",
      "\n",
      "  4. Now our equation looks simpler, with 'x' only on one side of the equals sign (i.e., \"2x = 4\"). The next step is to get rid of any coefficients that are multiplying x so we can isolate it completely: divide both sides by its coefficient (which in this case is 2).\n",
      "   (2x)/2 = 4/2\n",
      "    Simplify the division and you will find your solution!\n",
      "     x = 2\n",
      "\n",
      "So, to answer \"2x + 3 = 7\", we followed these steps: subtracted 3 from both sides of the equation; then divided by 2. The value of 'x' that satisfies this equation is indeed 2 (i.e., if you substitute 'x=2' back into our original expression, it will yield a true statement!).\n",
      "    This method can be applied to any linear equation in order to solve for the variable(s) involved and find its/their value(-es), making this an essential skill set when working with mathematics."
     ]
    }
   ],
   "source": [
    "def initialize_llm():\n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "    return Ollama(base_url=\"http://localhost:11434\", model=\"unquantphi\", callback_manager=callback_manager)\n",
    "\n",
    "llm = initialize_llm()\n",
    "\n",
    "system_prompt = \"You are a helpful AI assistant that provides clear and concise information.\"\n",
    "user_prompt = \"Write step by step the answer to 2x+3=7.\"\n",
    "\n",
    "# Construct the full prompt using the Hugging Face format\n",
    "full_prompt = f\"<|system|> {system_prompt}<|end|><|user|> {user_prompt}<|end|><|assistant|> \"\n",
    "\n",
    "# Run the test\n",
    "print(\"Testing LLM with prompt:\")\n",
    "print(full_prompt)\n",
    "response = llm(full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_llm():\n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "    return Ollama(base_url=\"http://localhost:11434\", model=\"quantphi\", callback_manager=callback_manager)\n",
    "\n",
    "llm = initialize_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initalizing the text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=10000,  \n",
    "    chunk_overlap=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_template = \"\"\"<|system|>\n",
    "You are an AI assistant specialized in summarizing portions of YouTube video transcripts. Your task is to provide clear and concise summaries of transcript chunks.\n",
    "<|end|>\n",
    "<|user|>\n",
    "Please summarize the following portion of a YouTube video transcript. Focus on the main points and key ideas:\n",
    "\n",
    "{text}\n",
    "\n",
    "Provide a brief summary of this chunk.\n",
    "<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "map_prompt = ChatPromptTemplate.from_template(map_template)\n",
    "\n",
    "# Define the refinement template\n",
    "refine_template = \"\"\"<|system|>\n",
    "You are an AI assistant specialized in refining and structuring summaries of YouTube video transcripts. Your task is to create a comprehensive and well-organized final summary.\n",
    "<|end|>\n",
    "<|user|>\n",
    "We have summarized a YouTube video transcript in parts. Here's a summary of what we have so far:\n",
    "\n",
    "{existing_answer}\n",
    "\n",
    "We have some new information to add:\n",
    "\n",
    "{text}\n",
    "\n",
    "Please refine the existing summary by incorporating this new information. Ensure the final summary is well-structured and follows this format:\n",
    "\n",
    "1. Main Topic:\n",
    "2. Key Points:\n",
    "3. Notable Details:\n",
    "4. Conclusion or Call to Action (if any):\n",
    "\n",
    "If any section is not applicable, you may omit it.\n",
    "<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "refine_prompt = ChatPromptTemplate.from_template(refine_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the summarization chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=map_prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    "    input_key=\"input_documents\",\n",
    "    output_key=\"output_text\",\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing Youtube video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting YouTube Video's Audio...\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=E0Hmnixke2g\n",
      "[youtube] E0Hmnixke2g: Downloading webpage\n",
      "[youtube] E0Hmnixke2g: Downloading ios player API JSON\n",
      "[youtube] E0Hmnixke2g: Downloading web creator player API JSON\n",
      "[youtube] E0Hmnixke2g: Downloading m3u8 information\n",
      "[info] E0Hmnixke2g: Downloading 1 format(s): 18\n",
      "[download] Destination: Saved_Media\\video.mp4\n",
      "[download] 100% of   22.43MiB in 00:00:02 at 10.48MiB/s    \n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=E0Hmnixke2g\n",
      "[youtube] E0Hmnixke2g: Downloading webpage\n",
      "[youtube] E0Hmnixke2g: Downloading ios player API JSON\n",
      "[youtube] E0Hmnixke2g: Downloading web creator player API JSON\n",
      "[youtube] E0Hmnixke2g: Downloading m3u8 information\n",
      "[info] E0Hmnixke2g: Downloading 1 format(s): 251\n",
      "[download] Destination: Saved_Media\\audio.webm\n",
      "[download] 100% of   14.49MiB in 00:00:01 at 13.06MiB/s    \n",
      "Video and audio download completed!\n",
      "MoviePy - Writing audio in Saved_Media\\audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Audio conversion to MP3 completed!\n",
      "Transcribing Audio...\n",
      "Using device: cuda\n",
      "Summarizing YouTube transcript...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n",
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: <|system|>\n",
      "You are an AI assistant specialized in summarizing portions of YouTube video transcripts. Your task is to provide clear and concise summaries of transcript chunks.\n",
      "<|end|>\n",
      "<|user|>\n",
      "Please summarize the following portion of a YouTube video transcript. Focus on the main points and key ideas:\n",
      "\n",
      " In the next 17 minutes, I will give you an overview of the most important machine learning algorithms to help you decide which one is right for your problem. My name is Tim, and I have been a data scientist for over 10 years and taught all of these algorithms to hundreds of students in real life machine learning boot camps. There's a simple strategy for picking the right algorithm for your problem. In 17 minutes, you will know how to pick the right one for any problem and get a basic intuition of each algorithm and how they relate to each other. My goal is to give as many of you as possible an intuitive understanding of the major machine learning algorithms.  to make you stop feeling overwhelmed. According to Wikipedia, machine learning is a field of study in artificial intelligence, concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data and thus perform tasks without explicit instructions. Much of the recent advancements in AI are driven by neural networks, which I hope to give you an intuitive understanding of by the end of this video. Let's divide machine learning into its subfields. Generally machine learning is divided into two areas, supervised learning and unsupervised learning.  We have a data set with any number of independent variables, also called features or input variables. And it dependent variable, also called target or output variable that is supposed to be predicted. We have a so-called training data set where we know the true values for the output variable, also called labels, that we can train our algorithm on to later predict the output variable for new unknown data. Examples could be predicting the price of a house, the output variable based on features of the house, say square footage, location, year of construction, et cetera, categorizing an  object as a cat or a dog. The output variable or label based on features of the object, say height, weight size of the ears, color of the eyes, etc. Unsupervised learning is basically any learning problem that is not supervised. So we're no truth about the data is known. So we're a supervised algorithm would be like showing a little kid what a typical cat looks like and what a typical dog looks like and then giving it a new picture and asking it what animal it sees. An unsupervised algorithm would be giving a kid with no idea of what cats and dogs are. A pile of pictures of animals and asking it to group by similarity with  any further instructions. Examples of unsupervised problems might be to sort all of your emails into three unspecified categories, which you can then later inspect and name as you wish. The algorithm will decide on its own how it will create those categories, also called clusters. Let's start with supervised learning. Arguably the bigger and more important branch of machine learning. There are broadly two sub categories. In regression, we want to predict a continuous numeric target variable for a given input variable. Using the example from before, it could be predicting the price of a house given any number of  features of a house and determining their relationship to the final price of the house. We might, for example, find out that square footage is directly proportional to the price linear dependence, but that the age of the house has no influence on the price of the house. In classification, we try to assign a discrete categorical label, also called a class, to a data point. For example, we may want to assign the label spam or no spam to an email based on its content, sender, and so on. But we could also have more than two classes. For example, junk, primary social promotions and updates, as Gmail does by default.  to the actual algorithms, starting with the mother of all machine learning algorithms, linear regression. In general, supervised learning algorithms, try to determine the relationship between two variables. We try to find the function that maps one to the other. Linear regression in its simplest form is trying to determine a linear relationship between two variables, namely the input and the output. We want to fit a linear equation to the data by minimizing the sum of squares of the distances between data points and the regression line. This simply minimizes the average distance of the real data to  are predictive model in this case, the regression line. And should therefore minimize prediction errors for new data points. A simple example of a linear relationship might be the height and shoe size of a person, where the regression fit my tell us that for every one unit of shoe size increase, the person will be on average two inches taller. You can make your model more complex and fit multi-dimensional data to an output variable. In the example of the shoe size, you might for example want to include the gender, age, and ethnicity of the person to get an even better model. Many of the very fancy machine learning algorithms, including neural networks, are just extend  of this various in-bladier, as I will show you later in the video. Logistic regression is a variant of linear regression and probably the most basic classification algorithm. Instead of fitting a line to two numerical variables, with a presumably linear relationship, you now try to predict a categorical output variable using categorical or numerical input variables. Let's look at an example. We now want to predict one of two classes. For example, the gender of a person based on height and weight. So a linear regression wouldn't make much sense anymore. Instead of fitting a line of  to the data, we now fit a so-called sigmoid function to the data, which looks like this. The equation will now not tell us about a linear relationship between two variables, but will now conveniently tell us the probability of a data point falling into a certain class given the value of the input variable. So for example, the likelihood of an adult person with a height of 180 centimeters being a man would be 80%. This is completely made up, of course. The k nearest neighbor's algorithm, or k and n, is a very simple and intuitive algorithm that can be used for both regression and classification.  It is a so-called non-parametric algorithm. The name means that we don't try to fit any equations and thus find any parameters of a model. So no true model fitting is necessary. The idea of K&N is simply that for any given new data point we will predict the target to be the average of its k nearest neighbors. While this might seem very simple, this is actually a very powerful predictive algorithm. Especially when relationships are more complicated than a simple linear relationship. In a classification example, we might say that the gender of a person will be the same as the majority of the five people closest and wait and hide to the person in  In a regression example, we might say that the weight of a person is the average weight of the three people closest in height and of chest circumference. This makes a ton of intuitive sense. You might realize that the number three seems a bit arbitrary and it is. K is called a hyperparameter of the algorithm and choosing the right K is an art. Choosing a very small number of K, say one or two, will lead to your model predicting your training data set very well, but not generalizing well to unseen data. This is called overfitting. Choosing a very large number, say 1000, will lead to a worsted overall.  is called underfitting. The best number is somewhere in between and depends a lot on the problem at hand. Methods for finding the right hyper parameters include cross validation, but are beyond the scope of this video. Support vector machine is a supervised machine learning algorithm, originally designed for classification tasks, but it can also be used for regression tasks. The core concept of the algorithm is to draw a decision boundary between data points that separates data points of the training set as well as possible. As the name suggests, a new unseen data point will be classified according to where it falls,  with respect to the decision boundary. Let's take this arbitrary example of trying to classify animals by their weight and the length of their nose. In this simple case of trying to classify cats and elephants, the decision boundary is a straight line. The SVM algorithm tries to find the line that separates the classes with the largest margin possible that is maximizing the space between the different classes. This makes the decision boundary generalized well and less sensitive to noise and outliers in the training data. The so-called support factors are the data points that sit on the edge of the margin. Knowing the support factors is enough to classify new data points  which often makes the algorithm very memory efficient. One of the benefits of SVM is that it is very powerful in high dimensions. That is if the number of features is large compared to the size of the data. In those high dimensional cases, the decision boundary is called a hyperplane. Another feature that makes SVM's extremely powerful is the use of so-called kernel functions, which allow for the identification of highly complex non-linear decision boundaries. Kernel functions are an implicit way to turn your original features into new, more complex features using the so-called kernel trick, which is beyond the scope of the speed of the  This allows for efficient creation of non-linear decision boundaries by creating complex new features such as weight divided by height squared. Also called the BMI. This is called implicit feature engineering. Neural networks take the idea of implicit feature engineering to the next level as I will explain later.  We can train our algorithm with a number of spam and non spam emails and count the occurrences of different words in each class. And thereby calculate the probability of certain words appearing in spam emails and non spam emails. We can then quickly classify a new email based on the words it contains, by by using Bay's theorem. We simply multiply the different probabilities of all words in the email together. This algorithm makes the false assumption that the probabilities of the different words appearing are independent of each other, which is why we call this class a fire naive. This makes it very computationally efficient,  while still being a good approximation for many use cases such as spam classification and other text-based classification tasks. Decision trees are the basis of a number of more complex supervised learning algorithms. In its simplest form, a decision tree looks somewhat like this. The decision tree is basically a series of yes-no questions that allow us to partition a dataset in several dimensions. Here is an example, decision tree for classifying people into high and low-risk patients for heart attacks. The goal of the decision tree algorithm is to create so-called leaf nodes at the bottom of the tree  is pure as possible, meaning instead of randomly splitting the data, we try to find splits that lead to the resulting groups or leaves to be as pure as possible, which is to say that as few data points as possible are misclassified. While this might seem like a very basic and simple algorithm, which it is, we can turn it into a very powerful algorithm by combining many decision trees together. Combining many simple models to a more powerful, complex model is called an ensemble algorithm. One form of ensembling is bagging, where we train multiple models on different subsets of the training data using a method called bootstrapping.  A famous version of this idea is called a random forest, where many decision trees vote on the classification of your data by majority vote of the different trees in the random forest. Random forests are very powerful estimators that can be used both for classification and regression. The randomness comes from randomly excluding features for different trees in the forest, which prevents overfitting and makes it much more robust because it removes correlation between the trees. Another type of ensemble method is called boosting, where instead of running many decision trees in parallel like for random forests, we train models  in sequence. We can combine a series of weak models in sequence, thus becoming a strong model. Because each sequential model tries to fix the errors of the previous model, boosted trees, often get to higher accuracy than random forests, but are also more prone to overfitting. Its sequential nature makes it slower to train than random forests. Famous examples of boosted trees are a debutst, gradient boosting, and XG boost. The details of which are beyond the scope of this video. Now let's get to the raining king of AI neural networks. To understand neural  network, let's look at logistic regression again. Say we have a number of features and are trying to predict a target class. The features might be pixel intensities of a digital image and the target might be classifying the image as one of the digits from 0 to 9. Now for this particular case, you might see why this might be difficult to do with logistic regression because say the number one doesn't look the same when different people write it. And even if the same person writes it several times, it will look slightly different each time and it won't be the exact same pixels illuminated for every instance of the number one. All of the instances of the number one have commonalities. However, like  they all have a dominating vertical line and usually no crossing lines as other digits might have. And usually there are no circular shapes in the number one as there would be in the number eight or nine. However, the computer doesn't initially know about these more complex features, but only the pixel intensities. We could manually engineer these features by measuring some of these things and explicitly adding them as new features. But artificial neural networks, similarly to using a kernel function with a support vector machine, are designed to implicitly and automatically design these features for us, without any guidance from humans.  We do this by adding additional layers of unknown variables between the input and output variables. In its simplest form, this is called a single layer perceptron, which is basically just a multi-feature regression task. Now if we add a hidden layer, the hidden variables in the middle layer represent some hidden unknown features, and instead of predicting the target variable directly, we try to predict these hidden features with our input features, and then try to predict the target variables with our new hidden features. In our specific example, we might be able to say that every time several pixels are illuminated next to each other, they represent a whole  We don't usually know what the hidden features represent. We just train the neural network to predict the final target as well as possible. The hidden features we can design this way are limited in the case of the single hidden layer. But what if we add a layer and have the hidden layer predict another hidden layer? What if we now add even more layers? This is called deep learning and can result in  complex hidden features to that might represent all kinds of complex information in the pictures, like the fact that there is a face in the picture. However, we will usually not know what the hidden features mean. We just know that they result in good predictions. All we have talked about so far is supervised learning, where we wanted to predict a specific target variable using some input variables. However, sometimes we don't have anything specific to predict and just want to find some underlying structure in our data. That's where unsupervised learning comes in. A very common unsupervised problem is clustering.  It's easy to confuse clustering with classification, but they are conceptually very different. Classification is when we know the classes we want to predict and have training data with true labels available, shown as colors here. Like pictures of cats and dogs. Clustering is when we don't have any labels and want to find unknown clusters just by looking at the overall structure of the data and trying to find potential clusters in the data. For example, we might look at a two-dimensional data set that looks like this. Any human will probably easily see three clusters here. But it's not always as straightforward as your data might also look like  This is the most famous clustering algorithm called K means clustering. Just like for Canon, K is a hyper parameter and stands for the number of clusters you are looking for. Finding the right number of clusters again is an art and has a lot to do with your specific problem and some trial and error in domain knowledge might be required. This is beyond the scope of this video. K means is very simple. You start by randomly selecting centers for your K clusters and assigning all data points to the cluster center closest to them.  The clusters here are shown in blue and green. You then recalculate the cluster centers based on the data points now assigned to them. You can see the centers moving closer to the actual clusters. You then assign the data points again to the new cluster centers followed by recalculating the cluster centers. You repeat this process until the centers of the clusters have stabilized. Well, K means is the most famous and most common clustering algorithm? Other algorithms exist, including some where you don't need to specify the number of clusters, like hierarchical clustering and DB scan,  which can find clusters of arbitrary shape, but I won't discuss them here. The last type of algorithm I will leave you with is dimensionality reduction. The idea of dimensionality reduction is to reduce the number of features or dimensions of your dataset, keeping as much information as possible. Usually the group of algorithms does this by finding correlations between existing features and removing potentially redundant dimensions without losing much information. For example, do you really need a picture in high resolution to recognize the airplane in the picture or can you reduce the number of pixels in the image?  and the functionality reduction will give you information about the relationships within your existing features and it can also be used as a preprocessing step in your supervised learning algorithm. To reduce the number of features in your dataset and make the algorithm more efficient and robust. An example algorithm is principle component analysis or PCA. Let's say we are trying to predict types of fish based on several features like length, height, color and number of teeth. When looking at the correlations of the different features, we might find that height and length are strongly correlated and including both won't help the algorithm  much and might in fact hurt it by introducing noise. We can simply include a shape feature that is a combination of the two. This is actually extremely common in large datasets and allows us to reduce the number of features dramatically and still get good results. PCA does this by finding the directions in which most variants in the dataset is retained. In this example, the direction of most variants is a diagonal, this is called the first principle component or PC and can become our new shape feature. The second principle component is orthogonal to the first and only explains a small fraction of the variants of the  dataset and can thus be excluded from our dataset in this case. In large datasets, we can do this for all features and rank them by explain variants and exclude any principal components that don't contribute much to the variants and thus wouldn't help much in our ML model. This was all common machine learning algorithms explained. If you are overwhelmed and don't know which algorithm you need, here is a great cheat sheet by scikit-learn that will help you decide which algorithm is right for which type of problem. If you want to roadmap on how to learn machine learning, check out my video on that.\n",
      "\n",
      "Provide a brief summary of this chunk.\n",
      "<|end|>\n",
      "<|assistant|>\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Final Summary:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def summarize_transcript(transcript):\n",
    "    \n",
    "    chunks = text_splitter.split_text(transcript)\n",
    "    docs = [Document(page_content=chunk) for chunk in chunks]\n",
    "    \n",
    "    result = summarize_chain({\"input_documents\": docs}) \n",
    "    return result[\"output_text\"]\n",
    "\n",
    "print(\"Getting YouTube Video's Audio...\")\n",
    "process_youtube_video(\"https://www.youtube.com/watch?v=E0Hmnixke2g\")\n",
    "print(\"Transcribing Audio...\")\n",
    "transcriber = WhisperTranscriber()\n",
    "transcribed_text = transcriber.transcribe(r\"Saved_Media\\audio.mp3\")\n",
    "print(\"Summarizing YouTube transcript...\")\n",
    "summary = summarize_transcript(transcribed_text)\n",
    "print(\"\\nFinal Summary:\")\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc_sum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

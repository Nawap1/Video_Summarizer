{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Whisper Tiny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from typing import List, Tuple\n",
    "os.environ['HF_TOKEN'] = open('token.txt', 'r').read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Model and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 384)\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 384, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 384)\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=384, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\").to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Resample Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(file_path, target_sampling_rate=16000):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Audio file not found: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=target_sampling_rate, mono=True)\n",
    "        return audio, sr\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading audio file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "        -9.9069439e-08,  1.4549005e-06,  4.4094631e-06], dtype=float32),\n",
       " 16000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_audio(\"sample_audio.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_audio(audio, chunk_length=30, sampling_rate=16000):\n",
    "    chunk_size = chunk_length * sampling_rate\n",
    "    return [audio[i:i+chunk_size] for i in range(0, len(audio), chunk_size)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcribing the chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_chunk(chunk, sampling_rate=16000):\n",
    "    input_features = processor(chunk, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "    input_features = input_features.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features)\n",
    "    \n",
    "    return processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I want to start off by saying that I know I have an accent and please don't assume its Russian. More and more people are complaining about the Go Transcript test, about our rules, about the log acceptance rates, and to be honest, I've pretty much headed with all this whining. First of all, the tests have we've had so far, we're easy. All you have to do is listen carefully, read our guidelines and do some research. These things are in the job description.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio, sr = load_audio(\"sample_audio.mp3\")\n",
    "chunks = chunk_audio(audio)\n",
    "transcription = transcribe_chunk(chunks[0])\n",
    "transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcribing the entire audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(file_path, chunk_length=30):\n",
    "    audio, sampling_rate = load_audio(file_path)\n",
    "    chunks = chunk_audio(audio, chunk_length, sampling_rate)\n",
    "    \n",
    "    transcriptions = [transcribe_chunk(chunk, sampling_rate) for chunk in chunks]\n",
    "    return \" \".join(transcriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I want to start off by saying that I know I have an accent and please don't assume its Russian. More and more people are complaining about the Go Transcript test, about our rules, about the log acceptance rates, and to be honest, I've pretty much headed with all this whining. First of all, the tests have we've had so far, we're easy. All you have to do is listen carefully, read our guidelines and do some research. These things are in the job description.  Our clients don't always have good quality files. Some may have background noise. Some might be in a difficult accent. Some might contain terms you've never heard of, but we have to provide 98, 99% accuracy for all of our clients. We've tried to make sure that our communication with employees and employers is a good one. We're available on Facebook, Skype, live support, email, phone. We answer all kinds of questions regardless of how redundant  they are, or in some cases, even rude. We have at least four ways of explaining how everything works. We tweak the way we communicate information over and over again, in the hope that it be clear for everyone. And yet, it's not. We're not one of those called hearted companies that doesn't give a fuck about its employees. We work with people on a personal level, which is why it's so frustrating for us to see duplicate the tests or account selling happening on  This things are so disrespectful I can't even begin to say you know what it's wrong on so many levels I'm not over dramatic and I'm not over reacting this things happen and we can't allow this kinds of behaviors within our company you know what let me tell you a little story only three years ago this company got like three or four orders per week small ones We were a handful of transcribers and we didn't have  and look where we are now. For someone who comes to us for the first time, they think like, this site always looked like this. This always had this many orders. They always had these rules. This system in place. All of our services and nobody had to really work hard to get where they are now. Well, that bullshit. We worked our asses off. All you see here today has been done by the  We had a program and two jacks of all trades. We worked day in and day out. We didn't have time of performance. We added something new every day. We did research. We advertised. We risked all of our savings for the company and it paid out. Now we got from 20 or less transcribers to 2000. We have over 500 editors. We hire all the time. We don't turn anyone down when they're asking for a chance. But what they do with that chance is  is up to them.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_file = \"sample_audio.mp3\"\n",
    "transcription = transcribe(audio_file, chunk_length=30)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc_sum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

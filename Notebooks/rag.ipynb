{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KNYpe\\AppData\\Local\\Temp\\ipykernel_1288\\2740559791.py:5: DeprecationWarning: callback_manager is deprecated. Please use callbacks instead.\n",
      "  llm = initialize_llm()\n"
     ]
    }
   ],
   "source": [
    "def initialize_llm():\n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "    return Ollama(base_url=\"http://localhost:11434\", model=\"quantphi\", callback_manager=callback_manager)\n",
    "\n",
    "llm = initialize_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import databutton as db\n",
    "import re\n",
    "from io import BytesIO\n",
    "from typing import Tuple, List\n",
    "import pickle\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from pypdf import PdfReader\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from io import BytesIO\n",
    "from typing import List, Tuple\n",
    "# from PyPDF2 import PdfReader  # Make sure PyPDF2 is installed\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, page_content: str, metadata: dict = None):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata if metadata else {}\n",
    "\n",
    "def parse_pdf(file: BytesIO, filename: str) -> Tuple[List[str], str]:\n",
    "    pdf = PdfReader(file)\n",
    "    output = []\n",
    "    for page in pdf.pages:\n",
    "        text = page.extract_text() or \"\"  # Ensure text is not None\n",
    "        text = re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", text)\n",
    "        text = re.sub(r\"(?<!\\n\\s)\\n(?!\\s\\n)\", \" \", text.strip())\n",
    "        text = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", text)\n",
    "        output.append(text)\n",
    "    return output, filename\n",
    "\n",
    "def text_to_docs(text: List[str], filename: str) -> List[Document]:\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    \n",
    "    page_docs = [Document(page_content=page, metadata={\"filename\": filename}) for page in text]\n",
    "    \n",
    "    # Assign page numbers to documents\n",
    "    for i, doc in enumerate(page_docs):\n",
    "        doc.metadata[\"page\"] = i + 1\n",
    "\n",
    "    doc_chunks = []\n",
    "    for doc in page_docs:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(  # Ensure this class is defined\n",
    "            chunk_size=4000,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    "            chunk_overlap=0,\n",
    "        )\n",
    "        chunks = text_splitter.split_text(doc.page_content)  # Split the page into chunks\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_doc = Document(\n",
    "                page_content=chunk, \n",
    "                metadata={\"page\": doc.metadata[\"page\"], \"chunk\": i}\n",
    "            )\n",
    "            chunk_doc.metadata[\"source\"] = f\"{chunk_doc.metadata['page']}-{chunk_doc.metadata['chunk']}\"\n",
    "            chunk_doc.metadata[\"filename\"] = filename  # Add filename to metadata\n",
    "            doc_chunks.append(chunk_doc)\n",
    "    \n",
    "    return doc_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KNYpe\\Desktop\\Video_Summarizer\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Load a pre-trained model from Sentence Transformers\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Function to generate embeddings using Sentence Transformers\n",
    "def generate_embeddings(docs):\n",
    "    embeddings = model.encode([doc['page_content'] for doc in docs])  # Get embeddings\n",
    "    embeddings = embeddings.astype('float32')  # FAISS requires float32\n",
    "    return embeddings\n",
    "def docs_to_index(docs):\n",
    "    embeddings = generate_embeddings(docs)  # Generate embeddings\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])  # Create the FAISS index\n",
    "    index.add(embeddings)  # Add embeddings to the index\n",
    "    return index, embeddings\n",
    "def get_index_for_pdf(pdf_files, pdf_names):\n",
    "    documents = []\n",
    "    for pdf_file, pdf_name in zip(pdf_files, pdf_names):\n",
    "        text,filename = parse_pdf(BytesIO(pdf_file), pdf_name)\n",
    "        documents.extend(text_to_docs(text,filename))\n",
    "    index, embeddings = docs_to_index(documents)\n",
    "    return index, embeddings, documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectordb(files, filenames):\n",
    "    # set_openai_api_key(api_key)\n",
    "    # vectordb = get_index_for_pdf([file.getvalue() for file in files], filenames)\n",
    "    index, embeddings, documents = get_index_for_pdf(files, filenames)\n",
    "    # return vector_db\n",
    "    return index, embeddings, documents\n",
    "\n",
    "def ask_question(vectordb, question):\n",
    "    k = 3  # Number of results to retrieve\n",
    "    distances, indices = vectordb.search(generate_embeddings([question]), k)\n",
    "    search_results = [{\"page_content\": vectordb[idx]['page_content'], \"metadata\": vectordb[idx]['metadata']} for idx in indices[0]]\n",
    "\n",
    "    pdf_extract = \"\\n\".join([result['page_content'] for result in search_results])\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "        You are a helpful Assistant who answers to users questions based on multiple contexts given to you.\n",
    "        Keep your answer short and to the point.\n",
    "        The evidence are the context of the pdf extract with metadata.\n",
    "        Carefully focus on the metadata specially 'filename' and 'page' whenever answering.\n",
    "        Make sure to add filename and page number at the end of sentence you are citing to.\n",
    "        Reply \"Not applicable\" if text is irrelevant.\n",
    "        The PDF content is:\n",
    "        {pdf_extract}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = prompt_template.format(pdf_extract=pdf_extract)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    response = llm(messages)\n",
    "\n",
    "    result = \"\"\n",
    "    for chunk in response:\n",
    "        text = chunk.choices[0].get(\"delta\", {}).get(\"content\")\n",
    "        if text:\n",
    "            result += text\n",
    "    return result.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KNYpe\\Desktop\\Video_Summarizer\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from io import BytesIO\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize your sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Replace with your specific model\n",
    "\n",
    "def create_vectordb(files, filenames):\n",
    "    index, embeddings, documents = get_index_for_pdf(files, filenames)\n",
    "    return index, embeddings, documents\n",
    "\n",
    "def ask_question(vectordb, question):\n",
    "    index, embeddings, documents = vectordb  # Unpack the tuple correctly\n",
    "    k = 3  # Number of results to retrieve\n",
    "    \n",
    "    # Generate embeddings for the question correctly\n",
    "    question_embedding = generate_embeddings([Document(page_content=question)])  # Wrap question in Document\n",
    "\n",
    "    distances, indices = index.search(question_embedding, k)  # Use index for searching\n",
    "    print(indices)\n",
    "    \n",
    "    # Retrieve the results based on the indices returned\n",
    "    search_results = [{\"page_content\": documents[idx].page_content, \"metadata\": documents[idx].metadata} for idx in indices[0]]\n",
    "\n",
    "    pdf_extract = \"\\n\".join([result['page_content'] for result in search_results])\n",
    "\n",
    "    prompt_template = f\"\"\"\n",
    "        You are a helpful Assistant who answers to users' questions based on multiple contexts given to you.\n",
    "        Keep your answer short and to the point.\n",
    "        The evidence is the context of the PDF extract with metadata.\n",
    "        Carefully focus on the metadata, especially 'filename' and 'page' whenever answering.\n",
    "        Make sure to add filename and page number at the end of the sentence you are citing.\n",
    "        Reply \"Not applicable\" if text is irrelevant.\n",
    "        The PDF content is:\n",
    "        {pdf_extract}\n",
    "        and\n",
    "        The question is:\n",
    "        {question}\n",
    "    \"\"\"\n",
    "    print(prompt_template)\n",
    "\n",
    "    # Send the prompt as a single string\n",
    "    # response = llm(prompt_template)  # Ensure llm is defined in your context\n",
    "    response = llm(prompt=prompt_template, stop=[\"\\n\", \"Not applicable\"])  # Use keyword argument for prompt\n",
    "\n",
    "    # Assuming llm returns a string directly\n",
    "    result = response# Adjust according to the actual response format\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_embeddings(docs):\n",
    "    # Extract page_content from Document objects\n",
    "    embeddings = model.encode([doc.page_content for doc in docs])  # Use dot notation instead of subscript\n",
    "    embeddings = embeddings.astype('float32')  # FAISS requires float32\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def docs_to_index(docs):\n",
    "    embeddings = generate_embeddings(docs)  # Generate embeddings\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])  # Create the FAISS index\n",
    "    index.add(embeddings)  # Add embeddings to the index\n",
    "    return index, embeddings\n",
    "\n",
    "def get_index_for_pdf(pdf_files, pdf_names):\n",
    "    documents = []\n",
    "    for pdf_file, pdf_name in zip(pdf_files, pdf_names):\n",
    "        text, filename = parse_pdf(BytesIO(pdf_file), pdf_name)\n",
    "        documents.extend(text_to_docs(text, filename))\n",
    "    index, embeddings = docs_to_index(documents)\n",
    "    return index, embeddings, documents\n",
    "\n",
    "# Make sure you have the following functions defined:\n",
    "# - parse_pdf: Function to parse PDF files and extract text.\n",
    "# - text_to_docs: Function to convert extracted text to a document format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the FilePath Below to the required PDF(works only for PDF )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example PDF files and names\n",
    "pdf_file_paths = [\"../graphfill.pdf\"]  # Add paths to your PDF files\n",
    "pdf_files = [open(path, \"rb\").read() for path in pdf_file_paths]\n",
    "pdf_names = [os.path.basename(path) for path in pdf_file_paths]\n",
    "\n",
    "\n",
    "# Create vector database\n",
    "vectordb = create_vectordb(pdf_files, pdf_names)\n",
    "\n",
    "# Ask a question\n",
    "# question = \"What is the main topic of the documents?\"\n",
    "# response = ask_question(vectordb, question)\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14 10  7]]\n",
      "\n",
      "        You are a helpful Assistant who answers to users' questions based on multiple contexts given to you.\n",
      "        Keep your answer short and to the point.\n",
      "        The evidence is the context of the PDF extract with metadata.\n",
      "        Carefully focus on the metadata, especially 'filename' and 'page' whenever answering.\n",
      "        Make sure to add filename and page number at the end of the sentence you are citing.\n",
      "        Reply \"Not applicable\" if text is irrelevant.\n",
      "        The PDF content is:\n",
      "        [27] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding in style: a stylegan encoder for image-to-image translation. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2287–2296, 2021. [28] Weijing Shi and Raj Rajkumar. Point-gnn: Graph neural network for 3d object detection in a point cloud. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1711–1719, 2020. [29] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256–2265. PMLR, 2015. [30] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [31] Yuhang Song, Chao Yang, Zhe Lin, Xiaofeng Liu, Qin Huang, Hao Li, and C-C Jay Kuo. Contextual-based image inpainting: Infer, match, and translate. In Proceedings of the European Conference on Computer Vision (ECCV), pages 3–19, 2018. [32] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 2149– 2159, 2022. [33] Qiaoyu Tan, Ninghao Liu, and Xia Hu. Deep representation learning for social network analysis. Frontiers in big Data, 2:2, 2019. [34] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 9446–9454, 2018. [35] Shashikant Verma, Rajendra Nagar, and Shanmuganathan Raman. Fast semantic feature extraction using superpixels for soft segmentation. In Computer Vision and Image Processing: 4th International Conference, CVIP 2019, Jaipur, India, September 27–29, 2019, Revised Selected Papers, Part I 4, pages 61–72. Springer, 2020. [36] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image synthesis and semantic manipulation with conditional gans. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8798–8807, 2018. [37] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image synthesis and semantic manipulation with conditional gans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018. [38] Guo-Sen Xie, Jie Liu, Huan Xiong, and Ling Shao. Scaleaware graph neural network for few-shot semantic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5475–5484, 2021.[39] Wei Xiong, Jiahui Yu, Zhe Lin, Jimei Yang, Xin Lu, Connelly Barnes, and Jiebo Luo. Foreground-aware image inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5840– 5848, 2019. [40] Shunxin Xu, Dong Liu, and Zhiwei Xiong. E2i: Generative inpainting from edge to image. IEEE Transactions on Circuits and Systems for Video Technology, 31(4):1308–1322, 2020. [41] Xingqian Xu, Shant Navasardyan, Vahram Tadevosyan, Andranik Sargsyan, Yadong Mu, and Humphrey Shi. Image completion with heterogeneously filtered spectral hints. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 4591–4601, 2023. [42] Zhaoyi Yan, Xiaoming Li, Mu Li, Wangmeng Zuo, and Shiguang Shan. Shift-net: Image inpainting via deep feature rearrangement. In Proceedings of the European conference on computer vision (ECCV), pages 1–17, 2018. [43] Lumin Yang, Jiajie Zhuang, Hongbo Fu, Xiangzhi Wei, Kun Zhou, and Youyi Zheng\n",
      "ModelPlaces365 (512x512) CelebA-HQ (512x512) Narr ow Masks Medium Masks W ide Masks Narr ow Masks Medium Masks W ide Masks FID↓LPIPS ↓SSIM↑FID↓LPIPS ↓SSIM↑FID↓LPIPS ↓SSIM↑FID↓LPIPS ↓SSIM↑FID↓LPIPS ↓SSIM↑FID↓LPIPS ↓SSIM↑ GraphFill-Pix (Iterati ve)3.81 0.111 0.908 7.792 0.133 0.876 9.067 0.158 0.86 2.462 0.111 0.929 7.727 0.13 0.899 11.119 0.146 0.881 GraphFill-Pix (Non-Iterati ve)4.285 0.113 0.907 8.833 0.137 0.876 10.47 0.16 0.841 3.207 0.115 0.922 9.019 0.135 0.892 15.356 0.15 0.876 GraphFill-FFC (Iterati ve)4.987 0.132 0.91 8.787 0.134 0.877 11.095 0.162 0.841 4.934 0.133 0.919 8.571 0.143 0.895 14.832 0.156 0.883 GraphFill-FFC (Non-Iterati ve)5.804 0.135 0.892 9.37 0.14 0.872 11.559 0.169 0.848 5.07 0.145 0.919 9.254 0.148 0.89 15.645 0.161 0.879 Pix2Pix (Shallo w)5.068 0.114 0.909 9.194 0.139 0.875 13.652 0.17 0.849 4.075 0.136 0.911 9.395 0.144 0.897 18.52 0.169 0.882 Pix2Pix (Deep)3.288 0.108 0.91 5.816 0.127 0.881 8.927 0.152 0.857 3.602 0.123 0.927 7.328 0.137 0.904 11.763 0.153 0.887 FFCResNet (Shallo w)5.977 0.142 0.881 9.151 0.145 0.862 11.807 0.178 0.86 5.287 0.14 0.918 9.919 0.146 0.896 17.379 0.162 0.88 FFCResNet (Deep)2.976 0.105 0.912 5.297 0.125 0.882 7.919 0.149 0.858 3.947 0.124 0.932 6.614 0.134 0.907 8.772 0.142 0.893 Table 4. Ablation studies on the effect of GraphFill integration on Shallow Baselines: Notable performance improvements and competitive performance compared to deep counterparts. Ablation Studies. We conducted several ablation studies to evaluate the performance of GraphFill and its integration with two shallow variants of Refine Networks: Pix2Pix [37], and FFCResNet proposed by [32]. We also tested iterative graph-filling (as discussed in section 3.2) and noniterative graph-filling schemes. In the non-iterative scheme, we directly input the full-graph G′to the GraphFill Network with the adjacency matrix Acalculated from the connectivity information in E. The non-iterative graph-filling scheme does not involve the merger operation at every successive pyramid level. Instead, the output at every pyramid level sub-graph is converted to coarser images using the G2I layer and averaged for coarse to masked union operation needed before Refine Network. We quantitatively compare our coarse-to-finer inpainting variants in Table 4. The GraphFill neural network is trained for 10epochs to learn the coarser representation. This pre-trained GraphFill Network is combined with shallow Refine Networks, and the entire model is trained end-to-end for refinement. All variants listed in Table 4 are trained for 5epochs on the Places365 dataset [53] and 25epochs on the CelebAHQ dataset [13]. As evident in Table 4, shallow networks integrated with the GraphFill module generate competitive inpainting results despite having lower learning parameters than their deep counterparts. Also, the results indicate that the variant with Iterative GraphFill integrated with shallow Pix2Pix architecture performs the best on average on both the validation split of the Places365 and CelebA-HQ datasets. We evaluate the performance of the RRPG by creating a validation split where the masked region is constrained to a 256×256square patch within a 512×512resolution image. Table 2 compares the GraphFill inpainting method with and without the Resolution-Robust Pyramidal Graph (RRPG) approach. Visual comparison between the two graph filling inpainting techniques is depicted in Figure5. The RRPG is designed to reduce computational complexity while maintaining inpainting performance, allowing efficient processing of high-resolution images. Additional qualitative results on the RRPG approach and Non-Iterative GraphFill can be found in the suppl. material. 5. Conclusion This work introduces a novel framework for image inpainting based on deep graph learning and pyramidal graph construction\n",
      "Model #P ars Model #P ars GraphFill (Ours) 175K DeepFillv2[46] 4.2M GraphFill-Pix (Ours) 175K + 4.4M CRFill[50] 4.2M Pix2Pix (Shallow) 4.4M CoordFill[19] 34.4M Pix2Pix[37] (Deep) 45.6M Big LaMa[32] 45M FFCResNet (Shallow) 3.8M MA T[16] 62M FFCResNet[32] (Deep) 27M CoModGAN[51] 109M A OTGAN[49] 15.2M SHGAN[41] 159.6M Table 1. Total Number of learnable parameters in GraphFill, Refine Network baselines, and other existing methods. ModelMetrics FID↓LPIPS ↓SSIM ↑ GraphFill-Pix (Iterative) with RRPG 1.509 0.0301 0.981 GraphFill-Pix (Iterative) without RRPG 1.505 0.0298 0.980 GraphFill-Pix (Non-Iterative) with RRPG 1.719 0.0308 0.976 GraphFill-Pix (Non-Iterative) without RRPG 1.704 0.0307 0.979 Table 2. Comparison of the proposed GraphFill inpainting models with and without the Resolution-robust Pyramidal Graph (RRPG). Symbol ↑denotes larger values are better. This ablation study is validated with random masks on a reduced validation split of 5000 images from the Places365[53] dataset. 3.3. Resolution-Robust Pyramidal Graph To address the inpainting of high-resolution images, we propose a resolution-robust pyramidal graph construction approach for inpainting using GraphFill, as illustrated in Figure 4. Since undesired objects occupy a smaller region of the overall image size, we use an adaptive cropping approach that crops the input image around the masked area (see Figure 4(c)). Our pyramidal graph construction follows a similar procedure as described in section 3.2. However, we use images with an increased crop and a larger value of n∈Nat higher levels of the pyramid, generating finer superpixels as the level of the pyramid increases. The lowest pyramid level contains the coarsest sub-graph generated from the full-resolution image, and the highest level contains the finest sub-graph generated from the maximum possible cropping of the input image. We constrain the maximum cropping around the masked region to ensure the image size is not reduced below a certain threshold. In our experiments, we set Hc= 224 andWc= 224. The cropping parameters are saved at each pyramid level to enable proper merger operations in CoarseNet and stitching to obtain the final inpainted image. The Coarse to Masked Union Operation is performed on the original image cropped to the maximum possible extent, and the coarse output ˆCppredicted at the p-th level of the pyramid graph, prepresenting the total number of levels in the pyramid and contains the finest sub-graph. To refine the sub-graphs at higher pyramid levels in the resolution-robust pyramidal graph, the Merger Operation relies on the cropping information saved at each level. The predicted coarse image ˆCi−1is obtained by applying the G2I layer on ˆGi−1with the superpixel map Si−1. This coarseimage is then cropped using the cropping parameters at the i-th level of the pyramid, resulting in ˆC⊡ i−1. The I2G-layer then transforms ˆC⊡ i−1instead of ˆCi−1to obtain a finer subgraph ˆG↑ i−1corresponding to the i-th pyramid level, using the superpixel map Si. The Merger Operation refines the graph and facilitates the transfer of global context from the (i−1)-th pyramid level to the i-th pyramid level. Figure 4(d-g) shows predicted coarse output ˆC⊡ i−1ati-th level of pyramid. Figure 4(h) shows the averaged output of all ˆC⊡ i’s stitched with corresponding cropping parameters. Figure 4(i) shows the final refined output from Refine Network. 4. Results and Discussions Datasets. Our proposed network is trained and evaluated on the Places365 [53] and CelebA-HQ [13] datasets, which have 1.8 million and 30k images, respectively, in the training split and 10k and 5k images, respectively, in the validation split. To evaluate our model and compare it to other state-of-the-art models, we adopt a similar approach to [32]. Specifically, we use pre-generated narrow (NM), medium (MM), and wide masks (WM) for each image in the validation split to ensure a fair comparison of metrics. Results\n",
      "        and\n",
      "        The question is:\n",
      "        Who are the paper's author?\n",
      "    \n",
      " The authors of the paper described in your text are Kai Jin, Jianjie Huang, Sheng Liu, Wei Chen, Qi Zhang, Yongkui Yu, Xinqiao Bian, and Kewei Zhu. However, since I cannot access real-time databases to confirm the latest research papers as of my last update in April 2023, please verify this information by checking the actual paper or its citations on academic platforms like Google Scholar, IEEE Xplore, or PubMed for accurate authorship details."
     ]
    }
   ],
   "source": [
    "question = \"Who are the paper's author?\"\n",
    "response = ask_question(vectordb, question)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The authors of the paper described in your text are Kai Jin, Jianjie Huang, Sheng Liu, Wei Chen, Qi Zhang, Yongkui Yu, Xinqiao Bian, and Kewei Zhu. However, since I cannot access real-time databases to confirm the latest research papers as of my last update in April 2023, please verify this information by checking the actual paper or its citations on academic platforms like Google Scholar, IEEE Xplore, or PubMed for accurate authorship details.'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
